{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Extraction Test Script\n",
        "\n",
        "Script n√†y s·∫Ω:\n",
        "1. ƒê·ªçc th√¥ng tin CV t·ª´ Resume.json\n",
        "2. Duy·ªát t·ª´ng file PDF trong folder resumes\n",
        "3. G·ªçi API pdfController ƒë·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu\n",
        "4. So s√°nh k·∫øt qu·∫£ v·ªõi d·ªØ li·ªáu trong Resume.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "import pandas as pd\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base directory: /Users/gothartnguyen/Documents/Gothart/DAI_HOC_2021_2026/TOT_NGHIEP/Jobhunter_AiServer/Jobhunter_AiServer\n",
            "Resume JSON path: /Users/gothartnguyen/Documents/Gothart/DAI_HOC_2021_2026/TOT_NGHIEP/Jobhunter_AiServer/Jobhunter_AiServer/Resume.json\n",
            "Resumes folder: /Users/gothartnguyen/Documents/Gothart/DAI_HOC_2021_2026/TOT_NGHIEP/Jobhunter_AiServer/Jobhunter_AiServer/resumes\n",
            "API endpoint: http://localhost:3005/api/v1/pdf/extract\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "BASE_DIR = Path.cwd()\n",
        "RESUME_JSON_PATH = BASE_DIR / \"Resume.json\"\n",
        "RESUMES_FOLDER = BASE_DIR / \"resumes\"\n",
        "\n",
        "# API Configuration\n",
        "API_BASE_URL = \"http://localhost:3005\"\n",
        "API_EXTRACT_ENDPOINT = f\"{API_BASE_URL}/api/v1/pdf/extract\"\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Resume JSON path: {RESUME_JSON_PATH}\")\n",
        "print(f\"Resumes folder: {RESUMES_FOLDER}\")\n",
        "print(f\"API endpoint: {API_EXTRACT_ENDPOINT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì ƒê√£ ƒë·ªçc 3 CV t·ª´ Resume.json\n",
            "\n",
            "Danh s√°ch CV trong Resume.json:\n",
            "1. Pham Duc Thanh - duybaoandinh@gmail.com\n",
            "2. Nguyen Minh Triet - duybaoandinh@gmail.com\n",
            "3. Vo Thi My Linh - duybaoandinh@gmail.com\n"
          ]
        }
      ],
      "source": [
        "# ƒê·ªçc Resume.json\n",
        "def load_resume_data(json_path: Path) -> List[Dict[str, Any]]:\n",
        "    \"\"\"ƒê·ªçc v√† parse file Resume.json\"\"\"\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"‚úì ƒê√£ ƒë·ªçc {len(data)} CV t·ª´ Resume.json\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚úó Kh√¥ng t√¨m th·∫•y file: {json_path}\")\n",
        "        return []\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚úó L·ªói parse JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "resume_data = load_resume_data(RESUME_JSON_PATH)\n",
        "\n",
        "# Hi·ªÉn th·ªã th√¥ng tin c√°c CV\n",
        "if resume_data:\n",
        "    print(\"\\nDanh s√°ch CV trong Resume.json:\")\n",
        "    for idx, cv in enumerate(resume_data, 1):\n",
        "        print(f\"{idx}. {cv.get('full_name', 'N/A')} - {cv.get('email', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mapping file names:\n",
            "  Data_Engineer.pdf -> Pham Duc Thanh (duybaoandinh@gmail.com)\n",
            "  FullStack_Web_Developer.pdf -> Nguyen Minh Triet (duybaoandinh@gmail.com)\n",
            "  Front_End_Developer.pdf -> Vo Thi My Linh (duybaoandinh@gmail.com)\n"
          ]
        }
      ],
      "source": [
        "# Mapping file names v·ªõi CV data\n",
        "# D·ª±a tr√™n t√™n file v√† th√¥ng tin trong Resume.json\n",
        "FILE_TO_CV_MAPPING = {\n",
        "    \"Data_Engineer.pdf\": \"Pham Duc Thanh\",\n",
        "    \"FullStack_Web_Developer.pdf\": \"Nguyen Minh Triet\",\n",
        "    \"Front_End_Developer.pdf\": \"Vo Thi My Linh\"\n",
        "}\n",
        "\n",
        "def find_cv_by_name(name: str, resume_list: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"T√¨m CV trong danh s√°ch d·ª±a tr√™n t√™n\"\"\"\n",
        "    for cv in resume_list:\n",
        "        if cv.get('full_name') == name:\n",
        "            return cv\n",
        "    return None\n",
        "\n",
        "print(\"Mapping file names:\")\n",
        "for filename, name in FILE_TO_CV_MAPPING.items():\n",
        "    cv = find_cv_by_name(name, resume_data)\n",
        "    if cv:\n",
        "        print(f\"  {filename} -> {name} ({cv.get('email', 'N/A')})\")\n",
        "    else:\n",
        "        print(f\"  {filename} -> {name} (NOT FOUND)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì T√¨m th·∫•y 3 file PDF trong folder resumes\n",
            "  - Front_End_Developer.pdf (34.03 KB)\n",
            "  - Data_Engineer.pdf (33.81 KB)\n",
            "  - FullStack_Web_Developer.pdf (35.34 KB)\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra folder resumes\n",
        "def get_pdf_files(folder_path: Path) -> List[Path]:\n",
        "    \"\"\"L·∫•y danh s√°ch c√°c file PDF trong folder\"\"\"\n",
        "    if not folder_path.exists():\n",
        "        print(f\"‚úó Folder kh√¥ng t·ªìn t·∫°i: {folder_path}\")\n",
        "        return []\n",
        "    \n",
        "    pdf_files = list(folder_path.glob(\"*.pdf\"))\n",
        "    print(f\"‚úì T√¨m th·∫•y {len(pdf_files)} file PDF trong folder resumes\")\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"  - {pdf_file.name} ({pdf_file.stat().st_size / 1024:.2f} KB)\")\n",
        "    return pdf_files\n",
        "\n",
        "pdf_files = get_pdf_files(RESUMES_FOLDER)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Ki·ªÉm tra k·∫øt n·ªëi API...\n",
            "‚úì API server ƒëang ho·∫°t ƒë·ªông\n"
          ]
        }
      ],
      "source": [
        "# H√†m g·ªçi API extract PDF\n",
        "def extract_pdf_via_api(file_path: Path, api_url: str) -> Dict[str, Any]:\n",
        "    \"\"\"G·ªçi API ƒë·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ PDF\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüìÑ ƒêang x·ª≠ l√Ω: {file_path.name}\")\n",
        "        \n",
        "        # M·ªü file v√† g·ª≠i request\n",
        "        with open(file_path, 'rb') as f:\n",
        "            files = {'file': (file_path.name, f, 'application/pdf')}\n",
        "            data = {'pageNumber': 0}\n",
        "            \n",
        "            response = requests.post(api_url, files=files, data=data, timeout=120)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                if result.get('success'):\n",
        "                    print(f\"  ‚úì Tr√≠ch xu·∫•t th√†nh c√¥ng\")\n",
        "                    return result.get('data', {}).get('extractedData', {})\n",
        "                else:\n",
        "                    print(f\"  ‚úó API tr·∫£ v·ªÅ success=false: {result.get('message', 'Unknown error')}\")\n",
        "                    return None\n",
        "            elif response.status_code == 403:\n",
        "                print(f\"  ‚úó PDF Extractor ƒëang b·ªã v√¥ hi·ªáu h√≥a\")\n",
        "                return None\n",
        "            else:\n",
        "                print(f\"  ‚úó L·ªói HTTP {response.status_code}: {response.text}\")\n",
        "                return None\n",
        "                \n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"  ‚úó Timeout khi g·ªçi API\")\n",
        "        return None\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(f\"  ‚úó Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn API. ƒê·∫£m b·∫£o server ƒëang ch·∫°y t·∫°i {api_url}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó L·ªói: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Test k·∫øt n·ªëi API\n",
        "print(\"\\nüîç Ki·ªÉm tra k·∫øt n·ªëi API...\")\n",
        "try:\n",
        "    health_check = requests.get(f\"{API_BASE_URL}/api/v1/health\", timeout=5)\n",
        "    if health_check.status_code == 200:\n",
        "        print(f\"‚úì API server ƒëang ho·∫°t ƒë·ªông\")\n",
        "    else:\n",
        "        print(f\"‚ö† API server tr·∫£ v·ªÅ status {health_check.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn API: {e}\")\n",
        "    print(f\"  ƒê·∫£m b·∫£o server ƒëang ch·∫°y t·∫°i {API_BASE_URL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÑ ƒêang x·ª≠ l√Ω: Front_End_Developer.pdf\n",
            "  ‚úì Tr√≠ch xu·∫•t th√†nh c√¥ng\n",
            "\n",
            "üìÑ ƒêang x·ª≠ l√Ω: Data_Engineer.pdf\n",
            "  ‚úì Tr√≠ch xu·∫•t th√†nh c√¥ng\n",
            "\n",
            "üìÑ ƒêang x·ª≠ l√Ω: FullStack_Web_Developer.pdf\n",
            "  ‚úì Tr√≠ch xu·∫•t th√†nh c√¥ng\n",
            "\n",
            "‚úÖ ƒê√£ x·ª≠ l√Ω 3 file PDF\n"
          ]
        }
      ],
      "source": [
        "# Duy·ªát t·ª´ng file PDF v√† g·ªçi API\n",
        "results = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    # T√¨m CV t∆∞∆°ng ·ª©ng trong Resume.json\n",
        "    cv_name = FILE_TO_CV_MAPPING.get(pdf_file.name)\n",
        "    expected_cv = find_cv_by_name(cv_name, resume_data) if cv_name else None\n",
        "    \n",
        "    # G·ªçi API extract\n",
        "    extracted_data = extract_pdf_via_api(pdf_file, API_EXTRACT_ENDPOINT)\n",
        "    \n",
        "    # L∆∞u k·∫øt qu·∫£\n",
        "    result_entry = {\n",
        "        'filename': pdf_file.name,\n",
        "        'expected_cv_name': cv_name,\n",
        "        'extracted_data': extracted_data,\n",
        "        'expected_data': expected_cv,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    results.append(result_entry)\n",
        "\n",
        "print(f\"\\n‚úÖ ƒê√£ x·ª≠ l√Ω {len(results)} file PDF\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä So s√°nh: Front_End_Developer.pdf\n",
            "  CV: Vo Thi My Linh\n",
            "  Match rate: 83.3% (5/6)\n",
            "  Skills overlap: 100.0%\n",
            "  Projects count: 3 (expected: 3) ‚úì\n",
            "  Project name matches: 2/3 (66.7%)\n",
            "\n",
            "üìä So s√°nh: Data_Engineer.pdf\n",
            "  CV: Pham Duc Thanh\n",
            "  Match rate: 100.0% (6/6)\n",
            "  Skills overlap: 100.0%\n",
            "  Projects count: 3 (expected: 3) ‚úì\n",
            "  Project name matches: 3/3 (100.0%)\n",
            "\n",
            "üìä So s√°nh: FullStack_Web_Developer.pdf\n",
            "  CV: Nguyen Minh Triet\n",
            "  Match rate: 83.3% (5/6)\n",
            "  Skills overlap: 100.0%\n",
            "  Projects count: 3 (expected: 3) ‚úì\n",
            "  Project name matches: 3/3 (100.0%)\n"
          ]
        }
      ],
      "source": [
        "# So s√°nh k·∫øt qu·∫£ tr√≠ch xu·∫•t v·ªõi d·ªØ li·ªáu trong Resume.json\n",
        "\n",
        "def normalize_text(value):\n",
        "    if isinstance(value, str):\n",
        "        return value.strip().lower()\n",
        "    return value\n",
        "\n",
        "def compare_fields(extracted: Dict, expected: Dict, field: str) -> Dict[str, Any]:\n",
        "    \"\"\"So s√°nh m·ªôt field gi·ªØa extracted v√† expected\"\"\"\n",
        "    extracted_value = normalize_text(extracted.get(field))\n",
        "    expected_value = normalize_text(expected.get(field))\n",
        "    \n",
        "    match = extracted_value == expected_value\n",
        "    \n",
        "    return {\n",
        "        'field': field,\n",
        "        'extracted': extracted.get(field),\n",
        "        'expected': expected.get(field),\n",
        "        'match': match\n",
        "    }\n",
        "\n",
        "def compare_projects(extracted_projects: List[Dict[str, Any]], expected_projects: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"So s√°nh danh s√°ch projects\"\"\"\n",
        "    project_results = []\n",
        "    matched_names = 0\n",
        "    used_indices = set()\n",
        "    \n",
        "    for expected_proj in expected_projects:\n",
        "        expected_name_norm = normalize_text(expected_proj.get('project_name'))\n",
        "        match = None\n",
        "        match_index = None\n",
        "        \n",
        "        for idx, extracted_proj in enumerate(extracted_projects):\n",
        "            if idx in used_indices:\n",
        "                continue\n",
        "            if normalize_text(extracted_proj.get('project_name')) == expected_name_norm:\n",
        "                match = extracted_proj\n",
        "                match_index = idx\n",
        "                break\n",
        "        \n",
        "        expected_languages = set(normalize_text(lang) for lang in (expected_proj.get('languages') or []) if lang)\n",
        "        expected_responsibilities = set(normalize_text(resp) for resp in (expected_proj.get('responsibilities') or []) if resp)\n",
        "        \n",
        "        project_comp = {\n",
        "            'expected_project_name': expected_proj.get('project_name'),\n",
        "            'match_found': match is not None,\n",
        "            'extracted_project_name': match.get('project_name') if match else None,\n",
        "            'role_match': False,\n",
        "            'duration_match': False,\n",
        "            'team_size_match': False,\n",
        "            'description_match': False,\n",
        "            'languages_expected_count': len(expected_languages),\n",
        "            'languages_extracted_count': 0,\n",
        "            'languages_matched': 0,\n",
        "            'languages_overlap': 0,\n",
        "            'responsibilities_expected_count': len(expected_responsibilities),\n",
        "            'responsibilities_extracted_count': 0,\n",
        "            'responsibilities_matched': 0,\n",
        "            'responsibilities_overlap': 0\n",
        "        }\n",
        "        \n",
        "        if match:\n",
        "            used_indices.add(match_index)\n",
        "            matched_names += 1\n",
        "            extracted_languages = set(normalize_text(lang) for lang in (match.get('languages') or []) if lang)\n",
        "            extracted_responsibilities = set(normalize_text(resp) for resp in (match.get('responsibilities') or []) if resp)\n",
        "            \n",
        "            project_comp.update({\n",
        "                'role_match': normalize_text(match.get('role')) == normalize_text(expected_proj.get('role')),\n",
        "                'duration_match': normalize_text(match.get('duration')) == normalize_text(expected_proj.get('duration')),\n",
        "                'team_size_match': match.get('team_size') == expected_proj.get('team_size'),\n",
        "                'description_match': normalize_text(match.get('description')) == normalize_text(expected_proj.get('description')),\n",
        "                'languages_extracted_count': len(extracted_languages),\n",
        "                'languages_matched': len(expected_languages & extracted_languages),\n",
        "                'languages_overlap': len(expected_languages & extracted_languages) / max(len(expected_languages), 1) if expected_languages else 0,\n",
        "                'responsibilities_extracted_count': len(extracted_responsibilities),\n",
        "                'responsibilities_matched': len(expected_responsibilities & extracted_responsibilities),\n",
        "                'responsibilities_overlap': len(expected_responsibilities & extracted_responsibilities) / max(len(expected_responsibilities), 1) if expected_responsibilities else 0\n",
        "            })\n",
        "        \n",
        "        project_results.append(project_comp)\n",
        "    \n",
        "    project_name_match_rate = matched_names / max(len(expected_projects), 1) if expected_projects else 0\n",
        "    \n",
        "    return {\n",
        "        'project_comparisons': project_results,\n",
        "        'projects_name_match_count': matched_names,\n",
        "        'total_expected_projects': len(expected_projects),\n",
        "        'project_name_match_rate': project_name_match_rate,\n",
        "        'extracted_projects_extra': max(len(extracted_projects) - len(expected_projects), 0)\n",
        "    }\n",
        "\n",
        "def compare_cv_data(extracted: Dict, expected: Dict) -> Dict[str, Any]:\n",
        "    \"\"\"So s√°nh to√†n b·ªô d·ªØ li·ªáu CV\"\"\"\n",
        "    if not extracted or not expected:\n",
        "        return {'error': 'Missing data for comparison'}\n",
        "    \n",
        "    # C√°c field c·∫ßn so s√°nh\n",
        "    fields_to_compare = [\n",
        "        'full_name',\n",
        "        'email',\n",
        "        'phone_number',\n",
        "        'objective',\n",
        "        'university',\n",
        "        'certificate'\n",
        "    ]\n",
        "    \n",
        "    comparisons = {}\n",
        "    matches = 0\n",
        "    total = len(fields_to_compare)\n",
        "    \n",
        "    for field in fields_to_compare:\n",
        "        comp = compare_fields(extracted, expected, field)\n",
        "        comparisons[field] = comp\n",
        "        if comp['match']:\n",
        "            matches += 1\n",
        "    \n",
        "    # So s√°nh technical_skills (list)\n",
        "    extracted_skills = set(normalize_text(s) for s in (extracted.get('technical_skills') or []) if s)\n",
        "    expected_skills = set(normalize_text(s) for s in (expected.get('technical_skills') or []) if s)\n",
        "    skills_match = len(extracted_skills & expected_skills) / max(len(expected_skills), 1) if expected_skills else 0\n",
        "    \n",
        "    # So s√°nh projects chi ti·∫øt\n",
        "    extracted_projects_list = extracted.get('projects') or []\n",
        "    expected_projects_list = expected.get('projects') or []\n",
        "    project_comparison = compare_projects(extracted_projects_list, expected_projects_list)\n",
        "    \n",
        "    # So s√°nh s·ªë l∆∞·ª£ng projects\n",
        "    extracted_projects = len(extracted_projects_list)\n",
        "    expected_projects = len(expected_projects_list)\n",
        "    projects_count_match = extracted_projects == expected_projects\n",
        "    \n",
        "    return {\n",
        "        'field_comparisons': comparisons,\n",
        "        'matches': matches,\n",
        "        'total_fields': total,\n",
        "        'match_rate': matches / total if total > 0 else 0,\n",
        "        'skills_overlap': skills_match,\n",
        "        'projects_count_match': projects_count_match,\n",
        "        'extracted_projects_count': extracted_projects,\n",
        "        'expected_projects_count': expected_projects,\n",
        "        **project_comparison\n",
        "    }\n",
        "\n",
        "# Th·ª±c hi·ªán so s√°nh cho t·ª´ng file\n",
        "comparison_results = []\n",
        "\n",
        "for result in results:\n",
        "    if result['extracted_data'] and result['expected_data']:\n",
        "        comparison = compare_cv_data(result['extracted_data'], result['expected_data'])\n",
        "        comparison['filename'] = result['filename']\n",
        "        comparison['cv_name'] = result['expected_cv_name']\n",
        "        comparison_results.append(comparison)\n",
        "        \n",
        "        print(f\"\\nüìä So s√°nh: {result['filename']}\")\n",
        "        print(f\"  CV: {result['expected_cv_name']}\")\n",
        "        print(f\"  Match rate: {comparison['match_rate']*100:.1f}% ({comparison['matches']}/{comparison['total_fields']})\")\n",
        "        print(f\"  Skills overlap: {comparison['skills_overlap']*100:.1f}%\")\n",
        "        print(f\"  Projects count: {comparison['extracted_projects_count']} (expected: {comparison['expected_projects_count']}) {'‚úì' if comparison['projects_count_match'] else '‚úó'}\")\n",
        "        print(f\"  Project name matches: {comparison['projects_name_match_count']}/{comparison['total_expected_projects']} ({comparison['project_name_match_rate']*100:.1f}%)\")\n",
        "        if comparison['extracted_projects_extra']:\n",
        "            print(f\"  Extra extracted projects: {comparison['extracted_projects_extra']}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö† Kh√¥ng th·ªÉ so s√°nh {result['filename']} (thi·∫øu d·ªØ li·ªáu)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "File: Front_End_Developer.pdf\n",
            "CV: Vo Thi My Linh\n",
            "============================================================\n",
            "\n",
            "‚úì full_name:\n",
            "  Extracted: Vo Thi My Linh\n",
            "  Expected:  Vo Thi My Linh\n",
            "\n",
            "‚úó email:\n",
            "  Extracted: duybaoanich@gmail.com\n",
            "  Expected:  duybaoandinh@gmail.com\n",
            "\n",
            "‚úì phone_number:\n",
            "  Extracted: 0345678123\n",
            "  Expected:  0345678123\n",
            "\n",
            "‚úì objective:\n",
            "  Extracted: Front-End Developer specializing in building responsive, accessible, and high-performance web interfaces. Experienced in implementing scalable component architectures and modern JavaScript frameworks.\n",
            "  Expected:  Front-End Developer specializing in building responsive, accessible, and high-performance web interfaces. Experienced in implementing scalable component architectures and modern JavaScript frameworks.\n",
            "\n",
            "‚úì university:\n",
            "  Extracted: University of Technology and Education - The University of Da Nang\n",
            "  Expected:  University of Technology and Education - The University of Da Nang\n",
            "\n",
            "‚úì certificate:\n",
            "  Extracted: Meta Front-End Developer Professional Certificate\n",
            "  Expected:  Meta Front-End Developer Professional Certificate\n",
            "\n",
            "--- Project comparisons ---\n",
            "\n",
            "Project 1: NovaUI - Design System and Component Library\n",
            "  Match found: ‚úó\n",
            "  -> Kh√¥ng t√¨m th·∫•y project t∆∞∆°ng ·ª©ng trong d·ªØ li·ªáu tr√≠ch xu·∫•t\n",
            "\n",
            "Project 2: FlowDash - Project Management Dashboard\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: FLOWDASH - PROJECT MANAGEMENT DASHBOARD\n",
            "  Role match: ‚úì\n",
            "  Duration match: ‚úó\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 4/4 (100.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n",
            "\n",
            "Project 3: ShopEase - E-commerce Front-End Interface\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: SHOPEASE - E-COMMERCE FRONT-END INTERFACE\n",
            "  Role match: ‚úó\n",
            "  Duration match: ‚úó\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 4/4 (100.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n",
            "\n",
            "============================================================\n",
            "File: Data_Engineer.pdf\n",
            "CV: Pham Duc Thanh\n",
            "============================================================\n",
            "\n",
            "‚úì full_name:\n",
            "  Extracted: Pham Duc Thanh\n",
            "  Expected:  Pham Duc Thanh\n",
            "\n",
            "‚úì email:\n",
            "  Extracted: duybaoandinh@gmail.com\n",
            "  Expected:  duybaoandinh@gmail.com\n",
            "\n",
            "‚úì phone_number:\n",
            "  Extracted: 0345678123\n",
            "  Expected:  0345678123\n",
            "\n",
            "‚úì objective:\n",
            "  Extracted: Data Engineer with strong skills in data pipeline design, ETL, and real-time streaming. Passionate about building scalable data infrastructures.\n",
            "  Expected:  Data Engineer with strong skills in data pipeline design, ETL, and real-time streaming. Passionate about building scalable data infrastructures.\n",
            "\n",
            "‚úì university:\n",
            "  Extracted: Ho Chi Minh University of Technology\n",
            "  Expected:  Ho Chi Minh University of Technology\n",
            "\n",
            "‚úì certificate:\n",
            "  Extracted: Google Cloud Data Engineer\n",
            "  Expected:  Google Cloud Data Engineer\n",
            "\n",
            "--- Project comparisons ---\n",
            "\n",
            "Project 1: Retail Data Warehouse\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: Retail Data Warehouse\n",
            "  Role match: ‚úì\n",
            "  Duration match: ‚úó\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 3/3 (100.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n",
            "\n",
            "Project 2: LogStream\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: Logstream\n",
            "  Role match: ‚úì\n",
            "  Duration match: ‚úó\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 3/3 (100.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n",
            "\n",
            "Project 3: Healthcare ETL Platform\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: Healthcare ETL Platform\n",
            "  Role match: ‚úì\n",
            "  Duration match: ‚úó\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 3/3 (100.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n",
            "\n",
            "============================================================\n",
            "File: FullStack_Web_Developer.pdf\n",
            "CV: Nguyen Minh Triet\n",
            "============================================================\n",
            "\n",
            "‚úì full_name:\n",
            "  Extracted: Nguyen Minh Triet\n",
            "  Expected:  Nguyen Minh Triet\n",
            "\n",
            "‚úó email:\n",
            "  Extracted: duybaoancinh@gmail.com\n",
            "  Expected:  duybaoandinh@gmail.com\n",
            "\n",
            "‚úì phone_number:\n",
            "  Extracted: 0345678123\n",
            "  Expected:  0345678123\n",
            "\n",
            "‚úì objective:\n",
            "  Extracted: Fullstack Web Developer specializing in modern web technologies, microservice architecture, and cloud-native development. Experienced in building scalable, maintainable, and high-performance web applications.\n",
            "  Expected:  Fullstack Web Developer specializing in modern web technologies, microservice architecture, and cloud-native development. Experienced in building scalable, maintainable, and high-performance web applications.\n",
            "\n",
            "‚úì university:\n",
            "  Extracted: Posts and Telecommunications Institute of Technology\n",
            "  Expected:  Posts and Telecommunications Institute of Technology\n",
            "\n",
            "‚úì certificate:\n",
            "  Extracted: AWS Certified Developer - Associate\n",
            "  Expected:  AWS Certified Developer - Associate\n",
            "\n",
            "--- Project comparisons ---\n",
            "\n",
            "Project 1: SmartOrder - Restaurant Management System\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: SmartOrder - Restaurant Management System\n",
            "  Role match: ‚úì\n",
            "  Duration match: ‚úì\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 4/4 (100.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n",
            "\n",
            "Project 2: EduPath - Online Learning Platform\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: EduPath - Online Learning Platform\n",
            "  Role match: ‚úì\n",
            "  Duration match: ‚úó\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 3/4 (75.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n",
            "\n",
            "Project 3: CityMarket - Local E-commerce Web App\n",
            "  Match found: ‚úì\n",
            "  Extracted project name: CityMarket - Local E-commerce Web App\n",
            "  Role match: ‚úì\n",
            "  Duration match: ‚úó\n",
            "  Team size match: ‚úì\n",
            "  Description match: ‚úì\n",
            "  Languages matched: 3/4 (75.0%)\n",
            "  Responsibilities matched: 0/6 (0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Hi·ªÉn th·ªã chi ti·∫øt so s√°nh cho t·ª´ng CV (bao g·ªìm projects)\n",
        "if comparison_results:\n",
        "    for comp_result in comparison_results:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"File: {comp_result['filename']}\")\n",
        "        print(f\"CV: {comp_result['cv_name']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        for field, comp in comp_result['field_comparisons'].items():\n",
        "            status = \"‚úì\" if comp['match'] else \"‚úó\"\n",
        "            print(f\"\\n{status} {field}:\")\n",
        "            print(f\"  Extracted: {comp['extracted']}\")\n",
        "            print(f\"  Expected:  {comp['expected']}\")\n",
        "        \n",
        "        if comp_result.get('project_comparisons'):\n",
        "            print(\"\\n--- Project comparisons ---\")\n",
        "            for idx, proj_comp in enumerate(comp_result['project_comparisons'], 1):\n",
        "                match_icon = \"‚úì\" if proj_comp['match_found'] else \"‚úó\"\n",
        "                print(f\"\\nProject {idx}: {proj_comp['expected_project_name']}\")\n",
        "                print(f\"  Match found: {match_icon}\")\n",
        "                if proj_comp['match_found']:\n",
        "                    print(f\"  Extracted project name: {proj_comp['extracted_project_name']}\")\n",
        "                    print(f\"  Role match: {'‚úì' if proj_comp['role_match'] else '‚úó'}\")\n",
        "                    print(f\"  Duration match: {'‚úì' if proj_comp['duration_match'] else '‚úó'}\")\n",
        "                    print(f\"  Team size match: {'‚úì' if proj_comp['team_size_match'] else '‚úó'}\")\n",
        "                    print(f\"  Description match: {'‚úì' if proj_comp['description_match'] else '‚úó'}\")\n",
        "                    print(f\"  Languages matched: {proj_comp['languages_matched']}/{proj_comp['languages_expected_count']} ({proj_comp['languages_overlap']*100:.1f}%)\")\n",
        "                    print(f\"  Responsibilities matched: {proj_comp['responsibilities_matched']}/{proj_comp['responsibilities_expected_count']} ({proj_comp['responsibilities_overlap']*100:.1f}%)\")\n",
        "                else:\n",
        "                    print(\"  -> Kh√¥ng t√¨m th·∫•y project t∆∞∆°ng ·ª©ng trong d·ªØ li·ªáu tr√≠ch xu·∫•t\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong comparison_results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã T·ªïng h·ª£p k·∫øt qu·∫£:\n",
            "                       File           CV Name Match Rate Skills Overlap Projects Match Project Name Match Rate  Extracted Projects  Expected Projects\n",
            "    Front_End_Developer.pdf    Vo Thi My Linh      83.3%         100.0%              ‚úì                   66.7%                   3                  3\n",
            "          Data_Engineer.pdf    Pham Duc Thanh     100.0%         100.0%              ‚úì                  100.0%                   3                  3\n",
            "FullStack_Web_Developer.pdf Nguyen Minh Triet      83.3%         100.0%              ‚úì                  100.0%                   3                  3\n"
          ]
        }
      ],
      "source": [
        "# T·∫°o b·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£\n",
        "if comparison_results:\n",
        "    summary_data = []\n",
        "    for comp in comparison_results:\n",
        "        summary_data.append({\n",
        "            'File': comp['filename'],\n",
        "            'CV Name': comp['cv_name'],\n",
        "            'Match Rate': f\"{comp['match_rate']*100:.1f}%\",\n",
        "            'Skills Overlap': f\"{comp['skills_overlap']*100:.1f}%\",\n",
        "            'Projects Match': '‚úì' if comp['projects_count_match'] else '‚úó',\n",
        "            'Project Name Match Rate': f\"{comp['project_name_match_rate']*100:.1f}%\",\n",
        "            'Extracted Projects': comp['extracted_projects_count'],\n",
        "            'Expected Projects': comp['expected_projects_count']\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(summary_data)\n",
        "    print(\"\\nüìã T·ªïng h·ª£p k·∫øt qu·∫£:\")\n",
        "    print(df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L∆∞u k·∫øt qu·∫£ ra file JSON\n",
        "output_file = BASE_DIR / f\"extraction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "output_data = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'api_endpoint': API_EXTRACT_ENDPOINT,\n",
        "    'total_files': len(results),\n",
        "    'results': results,\n",
        "    'comparisons': comparison_results\n",
        "}\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "File: Front_End_Developer.pdf\n",
            "CV: Vo Thi My Linh\n",
            "============================================================\n",
            "\n",
            "--- Project 1: NovaUI - Design System and Component Library ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The system provided a unified UI component framework compatible with React applications using TypeScript for type safety.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Storybook was used for component documentation, interactive demos, and visual testing integration.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Design tokens were introduced to synchronize colors, typography, and spacing across projects.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Sass mixins and utility classes were implemented for customizable theming and responsiveness.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Automated visual regression testing was configured using Chromatic CI integration.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The library supported accessibility standards including ARIA roles and keyboard navigation compliance.\n",
            "\n",
            "--- Project 2: FlowDash - Project Management Dashboard ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  React was used to implement a modular architecture separating UI views, containers, and reusable components.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Redux Toolkit handled global state management and API data synchronization with backend endpoints.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  TailwindCSS was adopted for utility-first styling and consistent responsive layouts across screen sizes.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Data visualization components were developed using Recharts for performance analytics and progress tracking.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Vite provided a fast development environment with optimized HMR and bundling for production builds.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Cross-browser and mobile compatibility were verified through automated testing with Jest and Testing Library.\n",
            "\n",
            "--- Project 3: ShopEase - E-commerce Front-End Interface ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The platform utilized Next.js server-side rendering (SSR) for enhanced SEO and faster first contentful paint times.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  React Query was employed for data fetching and caching to improve perceived performance.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Styled Components enabled dynamic theming with light/dark mode and maintainable CSS-in-JS architecture.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  GraphQL APIs were integrated for efficient product and category queries.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Lazy loading and code splitting techniques were implemented to reduce initial load size.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Performance optimization was verified using Lighthouse metrics and Core Web Vitals tracking.\n",
            "\n",
            "============================================================\n",
            "File: Data_Engineer.pdf\n",
            "CV: Pham Duc Thanh\n",
            "============================================================\n",
            "\n",
            "--- Project 1: Retail Data Warehouse ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The project established automated ELT workflows using Apache Airflow for multi-source data ingestion and integration.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  BigQuery was utilized as the central data warehouse, optimized with partitioned tables and clustering to enhance query performance.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The architecture followed a modular pipeline structure separating extraction, staging, transformation, and serving layers.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  DBT models were applied to create standardized fact and dimension tables for analytical reporting.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Data visualization was implemented using Looker Studio to provide sales and customer behavior insights.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Cloud Storage buckets were used for intermediate data staging and archival layers in GCP.\n",
            "\n",
            "--- Project 2: LogStream ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The system ingested logs from distributed services through Apache Kafka for high-throughput message streaming.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Spark Structured Streaming processed and aggregated data streams in near real-time to detect anomalies and generate metrics.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Data persistence was managed using PostgreSQL with time-series partitioning for efficient storage and retrieval.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Avro schemas were introduced to maintain schema consistency across microservices and message producers.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Monitoring and alerting were configured using Prometheus and Grafana dashboards to track system performance.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The overall pipeline achieved high fault tolerance and horizontal scalability through containerized deployment.\n",
            "\n",
            "--- Project 3: Healthcare ETL Platform ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The platform implemented automated ETL pipelines for structured and semi-structured healthcare data ingestion into Snowflake.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  DBT was adopted for modular data transformation, model versioning, and documentation.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Data quality validation and lineage tracking were integrated to ensure compliance with healthcare data standards.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Audit mechanisms were included to monitor data freshness, transformation success rates, and anomaly detection.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Data marts were designed to support downstream predictive modeling and analytical workloads.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Continuous integration and delivery were handled through CI/CD pipelines for seamless deployment and testing.\n",
            "\n",
            "============================================================\n",
            "File: FullStack_Web_Developer.pdf\n",
            "CV: Nguyen Minh Triet\n",
            "============================================================\n",
            "\n",
            "--- Project 1: SmartOrder - Restaurant Management System ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The system was designed using a service-oriented architecture separating menu, order, and user management modules.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  PostgreSQL was employed as the main relational database with optimized indexing for large-scale transactional queries.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  RESTful APIs were implemented through Express.js for interoperability with mobile and web clients.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Containerized deployment was achieved using Docker Compose with CI/CD integration via GitHub Actions.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Frontend UI was built with React and Material UI to ensure responsive layouts and reusable components.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Real-time order updates were enabled using WebSocket-based communication channels.\n",
            "\n",
            "--- Project 2: EduPath - Online Learning Platform ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The platform utilized Next.js for server-side rendering and NestJS for modular backend services.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  MongoDB handled course data, student records, and access logs with Redis caching for fast retrieval.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  JWT-based authentication and role-based authorization were applied across the application.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Video lecture streaming was integrated through a CDN with secure URL expiration policies.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  A microservice architecture enabled independent scaling of user management and analytics services.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Monitoring and performance metrics were collected using Prometheus and Grafana dashboards.\n",
            "\n",
            "--- Project 3: CityMarket - Local E-commerce Web App ---\n",
            "\n",
            "Responsibilities:\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  The project implemented a modular Express backend serving product, payment, and shipping APIs.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  MySQL database design followed a normalized schema with foreign key relationships for product and order data.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Frontend development utilized React with Redux Toolkit to manage client-side state and asynchronous API calls.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Payment gateway integration supported Stripe and MoMo for secure transaction handling.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  Nginx reverse proxy was configured for load balancing and SSL termination.\n",
            "\n",
            "‚úó responsibility:\n",
            "  Extracted: ‚Äî\n",
            "  Expected:  CI/CD pipelines automated testing, linting, and deployment across development and production environments.\n"
          ]
        }
      ],
      "source": [
        "# So s√°nh chi ti·∫øt Responsibilities cho t·ª´ng project theo ƒë·ªãnh d·∫°ng t·ª´ng d√≤ng\n",
        "# D·ª±a tr√™n d·ªØ li·ªáu \"results\" ƒë√£ c√≥ (extracted_data v√† expected_data)\n",
        "\n",
        "def find_project_by_name(projects, name):\n",
        "    name_norm = normalize_text(name)\n",
        "    for p in projects or []:\n",
        "        if normalize_text(p.get('project_name')) == name_norm:\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "for result in results:\n",
        "    extracted = result.get('extracted_data') or {}\n",
        "    expected = result.get('expected_data') or {}\n",
        "    \n",
        "    expected_projects = expected.get('projects') or []\n",
        "    extracted_projects = extracted.get('projects') or []\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"File: {result.get('filename')}\")\n",
        "    print(f\"CV: {result.get('expected_cv_name')}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for idx, exp_proj in enumerate(expected_projects, 1):\n",
        "        print(f\"\\n--- Project {idx}: {exp_proj.get('project_name')} ---\")\n",
        "        matched_proj = find_project_by_name(extracted_projects, exp_proj.get('project_name'))\n",
        "        \n",
        "        exp_resps = exp_proj.get('responsibilities') or []\n",
        "        ext_resps = matched_proj.get('responsibilities') if matched_proj else []\n",
        "        \n",
        "        ext_norm_to_raw = {normalize_text(r): r for r in (ext_resps or [])}\n",
        "        ext_norm_set = set(ext_norm_to_raw.keys())\n",
        "        \n",
        "        if not exp_resps:\n",
        "            print(\"(Kh√¥ng c√≥ responsibilities trong d·ªØ li·ªáu k·ª≥ v·ªçng)\")\n",
        "        \n",
        "        print(\"\\nResponsibilities:\")\n",
        "        for r in exp_resps:\n",
        "            rn = normalize_text(r)\n",
        "            matched = rn in ext_norm_set\n",
        "            icon = '‚úì' if matched else '‚úó'\n",
        "            extracted_line = ext_norm_to_raw.get(rn)\n",
        "            print(f\"\\n{icon} responsibility:\")\n",
        "            print(f\"  Extracted: {extracted_line if extracted_line is not None else '‚Äî'}\")\n",
        "            print(f\"  Expected:  {r}\")\n",
        "        \n",
        "        # Li·ªát k√™ responsibilities th·ª´a (c√≥ trong extracted nh∆∞ng kh√¥ng c√≥ trong expected)\n",
        "        extra = [raw for norm, raw in ext_norm_to_raw.items() if norm not in set(normalize_text(x) for x in exp_resps)]\n",
        "        if extra:\n",
        "            print(\"\\n(Responsibilities th√™m trong d·ªØ li·ªáu tr√≠ch xu·∫•t kh√¥ng c√≥ trong k·ª≥ v·ªçng):\")\n",
        "            for er in extra:\n",
        "                print(f\"  + {er}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ K·∫øt qu·∫£ ƒë·ªô ch√≠nh x√°c theo t·ª´ng CV:\n",
            "                       File           CV Name Fields Match Skills Overlap Project Name Match Projects Count Match Languages Avg Overlap Overall Accuracy\n",
            "    Front_End_Developer.pdf    Vo Thi My Linh        83.3%         100.0%              66.7%                    ‚úì                 66.7%            83.3%\n",
            "          Data_Engineer.pdf    Pham Duc Thanh       100.0%         100.0%             100.0%                    ‚úì                100.0%           100.0%\n",
            "FullStack_Web_Developer.pdf Nguyen Minh Triet        83.3%         100.0%             100.0%                    ‚úì                 83.3%            93.3%\n",
            "\n",
            "‚úÖ ƒê·ªô ch√≠nh x√°c trung b√¨nh (Overall Accuracy Mean): 92.2%\n"
          ]
        }
      ],
      "source": [
        "# T·ªïng h·ª£p ƒë·ªô ch√≠nh x√°c (matching %) c·ªßa PDF Extractor\n",
        "# T√≠nh ƒëi·ªÉm t·ªïng h·ª£p cho t·ª´ng CV v√† ƒëi·ªÉm trung b√¨nh to√†n b·ªô \n",
        "from statistics import mean\n",
        "\n",
        "summary_rows = []\n",
        "\n",
        "if comparison_results:\n",
        "    for comp in comparison_results:\n",
        "        fields_score = comp.get('match_rate', 0.0)  # t·ªâ l·ªá match c√°c field c∆° b·∫£n\n",
        "        skills_score = comp.get('skills_overlap', 0.0)  # t·ªâ l·ªá overlap k·ªπ nƒÉng\n",
        "        proj_name_score = comp.get('project_name_match_rate', 0.0)  # t·ªâ l·ªá kh·ªõp t√™n project\n",
        "        proj_count_score = 1.0 if comp.get('projects_count_match') else 0.0  # kh·ªõp s·ªë l∆∞·ª£ng project\n",
        "        \n",
        "        # Trung b√¨nh languages_overlap tr√™n t·∫•t c·∫£ project (n·∫øu c√≥)\n",
        "        languages_overlaps = []\n",
        "        for p in comp.get('project_comparisons', []) or []:\n",
        "            if isinstance(p.get('languages_overlap'), (int, float)):\n",
        "                languages_overlaps.append(p['languages_overlap'])\n",
        "        lang_avg = mean(languages_overlaps) if languages_overlaps else 0.0\n",
        "        \n",
        "        # C√¥ng th·ª©c t·ªïng h·ª£p (lo·∫°i b·ªè Responsibilities)\n",
        "        components = [fields_score, skills_score, proj_name_score, proj_count_score, lang_avg]\n",
        "        overall_accuracy = mean(components)\n",
        "        \n",
        "        summary_rows.append({\n",
        "            'File': comp.get('filename'),\n",
        "            'CV Name': comp.get('cv_name'),\n",
        "            'Fields Match': f\"{fields_score*100:.1f}%\",\n",
        "            'Skills Overlap': f\"{skills_score*100:.1f}%\",\n",
        "            'Project Name Match': f\"{proj_name_score*100:.1f}%\",\n",
        "            'Projects Count Match': '‚úì' if proj_count_score == 1.0 else '‚úó',\n",
        "            'Languages Avg Overlap': f\"{lang_avg*100:.1f}%\",\n",
        "            'Overall Accuracy': overall_accuracy  # gi·ªØ d·∫°ng s·ªë ƒë·ªÉ t√≠nh trung b√¨nh, hi·ªÉn th·ªã sau\n",
        "        })\n",
        "    \n",
        "    # In b·∫£ng k·∫øt qu·∫£\n",
        "    df_summary = pd.DataFrame(summary_rows)\n",
        "    # Hi·ªÉn th·ªã Overall Accuracy d·∫°ng %\n",
        "    if not df_summary.empty:\n",
        "        df_print = df_summary.copy()\n",
        "        df_print['Overall Accuracy'] = (df_print['Overall Accuracy'] * 100).map(lambda x: f\"{x:.1f}%\")\n",
        "        print(\"\\nüéØ K·∫øt qu·∫£ ƒë·ªô ch√≠nh x√°c theo t·ª´ng CV:\")\n",
        "        print(df_print.to_string(index=False))\n",
        "        \n",
        "        # T√≠nh trung b√¨nh to√†n b·ªô\n",
        "        overall_mean = float(mean(df_summary['Overall Accuracy'])) if not df_summary.empty else 0.0\n",
        "        print(f\"\\n‚úÖ ƒê·ªô ch√≠nh x√°c trung b√¨nh (Overall Accuracy Mean): {overall_mean*100:.1f}%\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Ch∆∞a c√≥ comparison_results. H√£y ch·∫°y c√°c cell ph√≠a tr√™n tr∆∞·ªõc.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
